/**** Start of imports. If edited, may not auto-convert in the playground. ****/
var r4 = ee.FeatureCollection("users/thinkbetween/R4_AdminBound"),
    geometry = /* color: #d63000 */ee.Geometry.Polygon(
        [[[-111.71104410478176, 38.83657923666038],
          [-111.55448892900051, 38.832835159362396],
          [-111.56375864335598, 38.96776475098204],
          [-111.71482065507473, 38.96723088215578]]]);
/***** End of imports. If edited, may not auto-convert in the playground. *****/
//Map.addLayer(r4, {}, 'r4')
//Module imports
var getImageLib = require('users/USFS_GTAC/modules:getImagesLib.js');
///////////////////////////////////////////////////////////////////////////////
// Define user parameters:

// 1. Specify study area: Study area
// Can specify a country, provide a fusion table  or asset table (must add 
// .geometry() after it), or draw a polygon and make studyArea = drawnPolygon
var studyArea = r4.geometry().bounds();

// 2. Update the startJulian and endJulian variables to indicate your seasonal 
// constraints. This supports wrapping for tropics and southern hemisphere.
// startJulian: Starting Julian date 
// endJulian: Ending Julian date
var startJulian = 161;
var endJulian = 263; 

// 3. Specify start and end years for all analyses
// More than a 3 year span should be provided for time series methods to work 
// well. If using Fmask as the cloud/cloud shadow masking method, this does not 
// matter
var startYear = 1984;
var endYear = 2018;

// 4. Specify an annual buffer to include imagery from the same season 
// timeframe from the prior and following year. timeBuffer = 1 will result 
// in a 3 year moving window
var timebuffer = 0;

// 5. Specify the weights to be used for the moving window created by timeBuffer
//For example- if timeBuffer is 1, that is a 3 year moving window
//If the center year is 2000, then the years are 1999,2000, and 2001
//In order to overweight the center year, you could specify the weights as
//[1,5,1] which would duplicate the center year 5 times and increase its weight for
//the compositing method
var weights = [1];



// 6. Choose medoid or median compositing method. 
// Median tends to be smoother, while medoid retains 
// single date of observation across all bands
// If not exporting indices with composites to save space, medoid should be used
var compositingMethod = 'medoid';

// 7. Choose Top of Atmospheric (TOA) or Surface Reflectance (SR) 
// Specify TOA or SR
// Current implementation does not support Fmask for TOA
var toaOrSR = 'SR';

// 8. Choose whether to include Landat 7
// Generally only included when data are limited
var includeSLCOffL7 = false;

//9. Whether to defringe L5
//Landsat 5 data has fringes on the edges that can introduce anomalies into 
//the analysis.  This method removes them, but is somewhat computationally expensive
var defringeL5 = false;

// 10. Choose cloud/cloud shadow masking method
// Choices are a series of booleans for cloudScore, TDOM, and elements of Fmask
//Fmask masking options will run fastest since they're precomputed
//CloudScore runs pretty quickly, but does look at the time series to find areas that 
//always have a high cloudScore to reduce comission errors- this takes some time
//and needs a longer time series (>5 years or so)
//TDOM also looks at the time series and will need a longer time series
var applyCloudScore = false;
var applyFmaskCloudMask = true;

var applyTDOM = false;
var applyFmaskCloudShadowMask = true;

var applyFmaskSnowMask = false;

// 11. Cloud and cloud shadow masking parameters.
// If cloudScoreTDOM is chosen
// cloudScoreThresh: If using the cloudScoreTDOMShift method-Threshold for cloud 
//    masking (lower number masks more clouds.  Between 10 and 30 generally 
//    works best)
var cloudScoreThresh = 20;

//Whether to find if an area typically has a high cloudScore
//If an area is always cloudy, this will result in cloud masking omission
//For bright areas that may always have a high cloudScore
//but not actually be cloudy, this will result in a reduction of commission errors
//This procedure needs at least 5 years of data to work well
var performCloudScoreOffset = false;

// If performCloudScoreOffset = true:
//Percentile of cloud score to pull from time series to represent a minimum for 
// the cloud score over time for a given pixel. Reduces comission errors over 
// cool bright surfaces. Generally between 5 and 10 works well. 0 generally is a
// bit noisy but may be necessary in persistently cloudy areas
var cloudScorePctl = 10;

// zScoreThresh: Threshold for cloud shadow masking- lower number masks out 
//    less. Between -0.8 and -1.2 generally works well
var zScoreThresh = -1;

// shadowSumThresh: Sum of IR bands to include as shadows within TDOM and the 
//    shadow shift method (lower number masks out less)
var shadowSumThresh = 0.35;

// contractPixels: The radius of the number of pixels to contract (negative 
//    buffer) clouds and cloud shadows by. Intended to eliminate smaller cloud 
//    patches that are likely errors
// (1.5 results in a -1 pixel buffer)(0.5 results in a -0 pixel buffer)
// (1.5 or 2.5 generally is sufficient)
var contractPixels = 1.5; 

// dilatePixels: The radius of the number of pixels to dilate (buffer) clouds 
//    and cloud shadows by. Intended to include edges of clouds/cloud shadows 
//    that are often missed
// (1.5 results in a 1 pixel buffer)(0.5 results in a 0 pixel buffer)
// (2.5 or 3.5 generally is sufficient)
var dilatePixels = 2.5;

// 12. correctIllumination: Choose if you want to correct the illumination using
// Sun-Canopy-Sensor+C correction. Additionally, choose the scale at which the
// correction is calculated in meters.
var correctIllumination = false;
var correctScale = 250;//Choose a scale to reduce on- 250 generally works well

//13. Export params
//Whether to export composites
var exportComposites = false;

//Set up Names for the export
var outputName = 'Landsat';

//Provide location composites will be exported to
//This should be an asset folder, or more ideally, an asset imageCollection
var exportPathRoot = 'users/ianhousman/test/changeCollection';



//CRS- must be provided.  
//Common crs codes: Web mercator is EPSG:4326, USGS Albers is EPSG:5070, 
//WGS84 UTM N hemisphere is EPSG:326+ zone number (zone 12 N would be EPSG:32612) and S hemisphere is EPSG:327+ zone number
var crs = 'EPSG:5070';

//Specify transform if scale is null and snapping to known grid is needed
var transform = [30,0,-2361915.0,0,-30,3177735.0];

//Specify scale if transform is null
var scale = null;
///////////////////////////////////////////////////////////////////////
// End user parameters
///////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////
//Start function calls

////////////////////////////////////////////////////////////////////////////////
//Call on master wrapper function to get Landat scenes and composites
var lsAndTs = getImageLib.getLandsatWrapper(studyArea,startYear,endYear,startJulian,endJulian,
  timebuffer,weights,compositingMethod,
  toaOrSR,includeSLCOffL7,defringeL5,applyCloudScore,applyFmaskCloudMask,applyTDOM,
  applyFmaskCloudShadowMask,applyFmaskSnowMask,
  cloudScoreThresh,performCloudScoreOffset,cloudScorePctl,
  zScoreThresh,shadowSumThresh,
  contractPixels,dilatePixels,
  correctIllumination,correctScale,
  exportComposites,outputName,exportPathRoot,crs,transform,scale);

//Separate into scenes and composites for subsequent analysis
var processedScenes = lsAndTs[0];
var processedComposites = lsAndTs[1];

////////////////////////////////////////////////////////////////////////////////
// Load the study region, with a blue outline.
// Create an empty image into which to paint the features, cast to byte.
// Paint all the polygon edges with the same number and width, display.
var empty = ee.Image().byte();
var outline = empty.paint({
  featureCollection: studyArea,
  color: 1,
  width: 3
});
Map.addLayer(outline, {palette: '0000FF'}, "Study Area", false);
// Map.centerObject(studyArea, 6);
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

// Location for Google Drive exports
var exportDriveFolder =  'DriveFolderHere';

// Location for Asset exports, can be '' if exporting to root of your assets. 
var exportCollection =  'stand_age_data';

// define the sign of spectral delta for vegetation loss for the segmentation index - 
// NBR delta is negetive for vegetation loss, so -1 for NBR, 1 for band 5, -1 for NDVI, etc
var dist_dir = -1;

//Indicate the start and end years of the annual composites
var start_year = 1986;
var end_year = 2017;

//Define landtrendr parameters
var run_params = {
  maxSegments:            6,
  spikeThreshold:         0.9,
  vertexCountOvershoot:   3,
  preventOneYearRecovery: true,
  recoveryThreshold:      0.25,
  pvalThreshold:          0.05,
  bestModelProportion:    0.75,
  minObservationsNeeded:  6
};

//Create an NBR index for each annual composite
var nbr_collection = processedComposites.map(function(img){return img.addBands(img.normalizedDifference(['nir','swir2']).rename(['NBR']).multiply(dist_dir).float())}).select(['NBR']);
//Map.addLayer(nbr_collection, {min:-1, max:1}, 'nbr collection', false);

//Append the NBR collection to the landtrendr paramters
run_params.timeSeries = nbr_collection; 

//Run LandTrendr and add the results to the map
var lt_result = ee.Algorithms.TemporalSegmentation.LandTrendr(run_params);
//Map.addLayer(lt_result, {}, 'lt result', false);

// work with the outputs
var lt_array = lt_result.select([0]);
var lt_year = lt_array.arraySlice(0,0,1).arrayProject([1]);
var lt_fitted = lt_array.arraySlice(0,2,3).arrayProject([1]);

//Join the fitted values to the raw values and add it to the map
var fittedCollection = arrayToTimeSeries(lt_fitted,lt_year,ee.List.sequence(start_year,end_year),'LT_Fitted_NBR');
var jc = getImageLib.joinCollections(nbr_collection, ee.ImageCollection(fittedCollection), false);
Map.addLayer(jc, {}, 'joined collection', false);

// slice out the vertices
var vertex_mask = lt_array.arraySlice(0,3,4);
// mask out all values that do not correspond to a vertex
var vertices = lt_array.arrayMask(vertex_mask);

// break the array into a left and right side to get to from years for  calculating duration and magnitude
var left = vertices.arraySlice(1,0,-1);
var right = vertices.arraySlice(1,1,null);

var startYear = left.arraySlice(0, 0, 1);                       // get year dimension of LT data from the segment start vertices
var startVal = left.arraySlice(0, 2, 3).multiply(dist_dir);     // get spectral index dimension of LT data from the segment start vertices
var endYear = right.arraySlice(0, 0, 1);                        // get year dimension of LT data from the segment end vertices 
var endVal = right.arraySlice(0, 2, 3).multiply(dist_dir);      // get spectral index dimension of LT data from the segment end vertices 

var dur = endYear.subtract(startYear);       // subtract the segment start year from the segment end year to calculate the duration of segments 
var mag = endVal.subtract(startVal);         // substract the segment start index value from the segment end index value to calculate the delta of segments
var rate = mag.divide(dur);                  // calculate the rate of spectral change

// make an array that contains all the segment attributes
var segInfo = ee.Image.cat([startYear.add(1), endYear, startVal, endVal, mag, dur, rate]).toArray(0).mask(vertex_mask.mask());
//Map.addLayer(segInfo, {}, 'segment info', false);

// filter by magnitude
var mag_size = -0.2;
var mag_mask = segInfo.arraySlice(0,4,5).lte(mag_size);
var segInfoMasked = segInfo.toArray(0).arrayMask(mag_mask);
//Map.addLayer(segInfoMasked, {}, 'segment info masked', false);

//get the most recent year of disturbance that met the magnitude threshold
var yod = segInfoMasked.arraySlice(0,1,2).arrayProject([1]).arrayReduce(ee.Reducer.max(),[0]).arrayFlatten([['yod']]);
Map.addLayer(yod.updateMask(yod.neq(0)), {min:1986, max:2018, palette:'FF0,F00'}, 'year of most recent disturbance');

// function to add normalized difference to collection
function normalizedDifference(img){
  img = img.addBands(img.normalizedDifference(['nir', 'red']).rename('ndvi'));
  img = img.addBands(img.normalizedDifference(['nir', 'swir2']).rename('nbr'));
  img = img.addBands(img.normalizedDifference(['green', 'swir1']).rename('ndii'));
  img = img.addBands(img.normalizedDifference(['nir', 'swir1']).rename('ndwi2'));
  return img;
}

//Function to convert an image array object to collection
function arrayToTimeSeries(tsArray,yearsArray,possibleYears,bandName){
    //Set up dummy image for handling null values
    var noDateValue = -32768;
    var dummyImage = ee.Image(noDateValue).toArray();
    
    //Ierate across years
    var tsC = possibleYears.map(function(yr){
      yr = ee.Number(yr);
      
      //Pull out given year
      var yrMask = yearsArray.eq(yr);
    
      //Mask array for that given year
      var masked = tsArray.arrayMask(yrMask);
      
      
      //Find null pixels
      var l = masked.arrayLength(0);
      
      //Fill null values and convert to regular image
      masked = masked.where(l.eq(0),dummyImage).arrayGet([-1]);
      
      //Remask nulls
      masked = masked.updateMask(masked.neq(noDateValue)).rename([bandName])      
        .set('system:time_start',ee.Date.fromYMD(yr,1,1).millis());
        
      return masked;
    
    
  });
  return ee.ImageCollection(tsC);
  }